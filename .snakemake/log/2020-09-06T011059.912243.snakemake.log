Building DAG of jobs...
Using shell: /bin/bash
Provided cores: 1
Rules claiming more threads will be scaled down.
Job counts:
	count	jobs
	1	all
	1	trim_reads_pe
	2

[Sun Sep  6 01:11:00 2020]
rule trim_reads_pe:
    input: /tier4/DSC/GATK/ehlim/gatk_germline_pipeline/resources/samples/BRCA_NA12892_R1.fastq.gz, /tier4/DSC/GATK/ehlim/gatk_germline_pipeline/resources/samples/BRCA_NA12892_R2.fastq.gz
    output: trimmed/BRCA_NA12892.1.fastq.gz, trimmed/BRCA_NA12892.2.fastq.gz, trimmed/BRCA_NA12892.1.unpaired.fastq.gz, trimmed/BRCA_NA12892.2.unpaired.fastq.gz
    log: logs/trimmomatic/BRCA_NA12892.log
    jobid: 1
    wildcards: sample=BRCA_NA12892

[Sun Sep  6 01:13:26 2020]
Finished job 1.
1 of 2 steps (50%) done

[Sun Sep  6 01:13:26 2020]
localrule all:
    input: trimmed/BRCA_NA12892.1.fastq.gz, trimmed/BRCA_NA12892.2.unpaired.fastq.gz
    jobid: 0

[Sun Sep  6 01:13:26 2020]
Finished job 0.
2 of 2 steps (100%) done
Complete log: /tier4/DSC/GATK/ehlim/gatk_germline_pipeline/.snakemake/log/2020-09-06T011059.912243.snakemake.log
