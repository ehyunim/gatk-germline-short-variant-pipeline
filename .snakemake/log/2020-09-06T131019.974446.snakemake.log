Building DAG of jobs...
Using shell: /bin/bash
Provided cores: 10
Rules claiming more threads will be scaled down.
Job counts:
	count	jobs
	1	all
	1	trim_reads_pe
	2

[Sun Sep  6 13:10:20 2020]
rule trim_reads_pe:
    input: /tier4/DSC/GATK/ehlim/gatk_germline_pipeline/resources/samples/BRCA_NA12892_R1.fastq.gz, /tier4/DSC/GATK/ehlim/gatk_germline_pipeline/resources/samples/BRCA_NA12892_R2.fastq.gz
    output: results/trimmed/BRCA_NA12892.1.fastq.gz, results/trimmed/BRCA_NA12892.2.fastq.gz, results/trimmed/BRCA_NA12892.1.unpaired.fastq.gz, results/trimmed/BRCA_NA12892.2.unpaired.fastq.gz
    log: logs/trimmomatic/BRCA_NA12892.log
    jobid: 1
    wildcards: sample=BRCA_NA12892
    threads: 8

[Sun Sep  6 13:11:25 2020]
Finished job 1.
1 of 2 steps (50%) done

[Sun Sep  6 13:11:25 2020]
localrule all:
    input: results/trimmed/BRCA_NA12892.1.fastq.gz, results/trimmed/BRCA_NA12892.2.unpaired.fastq.gz
    jobid: 0

[Sun Sep  6 13:11:25 2020]
Finished job 0.
2 of 2 steps (100%) done
Complete log: /tier4/DSC/GATK/ehlim/gatk_germline_pipeline/.snakemake/log/2020-09-06T131019.974446.snakemake.log
