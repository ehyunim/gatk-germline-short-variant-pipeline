Building DAG of jobs...
Using shell: /bin/bash
Provided cores: 8
Rules claiming more threads will be scaled down.
Job counts:
	count	jobs
	1	all
	1	mark_duplicates
	1	sort_bam
	3

[Wed Sep  9 14:50:26 2020]
rule sort_bam:
    input: results/mapped/BRCA_NA12892_rg.bam
    output: results/sorted/BRCA_NA12892.sorted.bam
    log: logs/samtools/sort/BRCA_NA12892.log
    jobid: 2
    wildcards: sample=BRCA_NA12892

[Wed Sep  9 14:50:45 2020]
Finished job 2.
1 of 3 steps (33%) done

[Wed Sep  9 14:50:45 2020]
rule mark_duplicates:
    input: results/sorted/BRCA_NA12892.sorted.bam
    output: results/dedup/BRCA_NA12892.dedup.bam, results/dedup/BRCA_NA12892.metrics.txt
    log: logs/picard/dedup/BRCA_NA12892.log
    jobid: 1
    wildcards: sample=BRCA_NA12892

[Wed Sep  9 14:51:31 2020]
Finished job 1.
2 of 3 steps (67%) done

[Wed Sep  9 14:51:31 2020]
localrule all:
    input: results/dedup/BRCA_NA12892.dedup.bam
    jobid: 0

[Wed Sep  9 14:51:31 2020]
Finished job 0.
3 of 3 steps (100%) done
Complete log: /tier4/DSC/GATK/ehlim/gatk_germline_pipeline/workflow/.snakemake/log/2020-09-09T145025.798290.snakemake.log
